---
title: "P8105 - Homework 6"
author: "Joe LaRocca"
date: "2024-11-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
library(glmnet)
library(modelr)
```

## Problem 1

### Upload Weather Data

```{r}

weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

```

### Create Function to Return Log(b_0 * b_1)

```{r}

bootstrap_log = function(lm) {
  log_b0_b1 = lm |>
    broom::tidy() |>
    pull(estimate) |>
    prod() |>
    log()
  return(log_b0_b1)
}

```

### Create Function to Return R^2 value

```{r}

bootstrap_rsq = function(lm) {
  r_squared = lm |>
    broom::glance() |>
    pull(r.squared)
  return(r_squared)
}

```

### Bootstrap 5000 Samples

```{r}

bootstraps_weather = weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    log_b0_b1 = map(models, bootstrap_log),
    r_sq = map(models, bootstrap_rsq)
  ) |>
  select(-strap, -models) |>
  unnest(log_b0_b1) |>
  unnest(r_sq)
  
```

### Plot the Distribution of log_b0_b1

```{r}

bootstraps_weather |>
  ggplot(aes(x = log_b0_b1)) + 
  geom_histogram(fill = "red", col = "black") + 
  labs(
    title = "Distribution of Log(b0 + b1) in a Bootstrapped Sample of 5000 SLRs",
    x = "Log(b0 * b1)",
    y = "Count"
  )

```

### Plot the Distribution of r_sq

```{r}

bootstraps_weather |>
  ggplot(aes(x = r_sq)) + 
  geom_histogram(fill = "lightblue", col = "black") + 
    labs(
    title = "Distribution of R^2 in a Bootstrapped Sample of 5000 SLRs",
    x = "R^2",
    y = "Count"
  )

```

We can see that both of these plots look to have an approximately Normal distribution, i.e. the distributions of $log(b_0 * b_1)$ and $R^2$ are approximately Normal in our sample of 5000 linear models.

### Identifying Quantiles for Confidence Intervals

```{r}

bootstraps_weather |>
  summarize(
    ci_lower_log_b0_b1 = quantile(log_b0_b1, 0.025),
    ci_upper_log_b0_b1 = quantile(log_b0_b1, 0.975),
    ci_lower_r_sq = quantile(r_sq, 0.025),
    ci_upper_r_sq = quantile(r_sq, 0.975)
  ) |>
  knitr::kable(digits = 3)

```

We can see that the 95\% confidence interval for $log(b_0 * b_1)$ is (1.967, 2.058), while the 95\% confidence interval for $R^2$ is (0.894, 0.927).

## Problem 2

### Upload and Filter Data

```{r}

homicides = read_csv("data/homicide-data.csv") |> 
  unite(col = "city_state", city, state, sep = ", ") |>
  filter(
    city_state != "Dallas, TX",
    city_state != "Phoenix, AZ",
    city_state != "Kansas City, MO",
    city_state != "Tulsa, AL",
    victim_race == "White" | victim_race == "Black",
    victim_sex == "Male" | victim_sex == "Female",
  ) |> 
  mutate(victim_age = as.numeric(victim_age)) |>
  drop_na(victim_age) |>
  mutate(unsolved = 
      disposition == "Closed by arrest")

```

### Create a Function to Return Odds Ratios (ORs) and CIs

```{r}

glm_sex_OR = function(glm){
  glm |> broom::tidy() |> 
  mutate(
    OR_estimate = exp(estimate),
    OR_CI_low = exp(estimate - 1.96 * std.error),
    OR_CI_high = exp(estimate + 1.96 * std.error)
    ) |>
  filter(term == "victim_sexMale") |>
  select(OR_estimate, OR_CI_low, OR_CI_high)
}

```

### Run a GLM for Baltimore Only

```{r}

homicides_baltimore = homicides |>
  filter(city_state == "Baltimore, MD") 

logit_model = glm(unsolved ~ victim_race + victim_age + victim_sex,
      data = homicides_baltimore, family = "binomial")

logit_model |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    OR_CI_low = exp(estimate - 1.96 * std.error),
    OR_CI_high = exp(estimate + 1.96 * std.error)
    ) |>
  filter(term == "victim_sexMale") |>
  select(OR, OR_CI_low, OR_CI_high)

```

### Run a GLM for All Cities

```{r}

homicides_ORs = homicides |>
  nest(data = -city_state) |>
  mutate(
    models = map(data, \(df) glm(unsolved ~ victim_age + victim_race + victim_sex, 
                      data = df,
                      family = "binomial")),
    results = map(models, glm_sex_OR) 
    ) |> 
  select(city_state, results) |>
  unnest(results) 

```

### Make a Plot of Cities by OR

```{r}

homicides_ORs |>
  ggplot(aes(x = fct_reorder(city_state, OR_estimate), y = OR_estimate)) +
  geom_boxplot() +
  geom_errorbar(aes(ymin = OR_CI_low, ymax = OR_CI_high)) +
  coord_flip() +
  labs(
    title = "Odds Ratios between Males/Females for Solved Homicides Across the U.S.",
    x = "City",
    y = "Estimated Odds Ratio of Homicide Being Solved"
  ) 

```

From the plot, we can see that generally, homicides with male victims were less likely to be solved than those with female victims, since the odds ratios for most cities are less than 1. The lowest estimated odds ratio, by far, is in New York City, where the estimate is just over 0.25. That is, on average, male victims' homicides had about one-quarter the odds of being solved than female victims' homicides.

A few west-coast cities (Albuquerque, Stockton, and Fresno) actually had an odds ratio greater than 1, implying that homicides with male victims were **more** likely to be solved than those with female victims.

## Problem 3

```{r}

birthweight = read_csv("data/birthweight.csv") |>
  mutate(
    ppbmi = round(ppbmi, 2),
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  )

```

### Use LASSO to narrow down the total number of coefficients

```{r}

set.seed(10)

y = birthweight$bwt
x = birthweight |>
  select(-bwt) |>
  data.matrix()

cv_model = cv.glmnet(x, y, alpha = 1)
lambda_min = cv_model$lambda.min

best_model = glmnet(x, y, alpha = 1, lambda = lambda_min)
coef(best_model)

```

### Create an OLS model with the predictors chosen by LASSO and Plot Residuals vs. Fitted Values

```{r}

my_model = lm(bwt ~ babysex + bhead + blength + delwt + 
                                      fincome + frace + gaweeks + menarche + 
                                      mheight + momage + mrace + parity + 
                                      smoken + wtgain,
                                    data = birthweight)

birthweight |> 
  add_predictions(my_model) |>
  add_residuals(my_model) |>
   ggplot(aes(x = pred, y = resid)) +
  geom_point() + 
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Residuals vs. Fitted Values Plot for My Model"
  )
 

```

In order to select predictors for my "optimal" model, I used LASSO to select a certain number of predictors, and then created an OLS model with the predictors that LASSO selected.


### Find RMSEs and plot them

```{r}

cv_df_bwt = crossv_mc(birthweight, 1000)

cv_df_bwt = 
  cv_df_bwt |> 
  mutate(
    my_model  = map(train, \(df) lm(bwt ~ babysex + bhead + blength + delwt + 
                                      fincome + frace + gaweeks + menarche + 
                                      mheight + momage + mrace + parity + 
                                      smoken + wtgain,
                                    data = df)),
    samp_1  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    samp_2  = map(train, \(df) lm(bwt ~ bhead + blength + babysex + 
                      bhead*blength*babysex, data = df))) |> 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_samp_1 = map2_dbl(samp_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_samp_2 = map2_dbl(samp_2, test, \(mod, df) rmse(model = mod, data = df)))

cv_df_bwt

```

### Make the Plot

```{r}

cv_df_bwt |>
  select(starts_with("rmse")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
   mutate(model = fct_inorder(model)) |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    x = "Model",
    y = "RMSE",
    title = "Distribution of RMSE for Birth Weight by Model Choice"
  )

```

From these violin plots, we can see from the RMSE distributions that the RMSE of "my" model is generally lower than that of either of the two sample models. 


